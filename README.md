# ğŸ“š PDF RAG 

Este projeto implementa **trÃªs arquiteturas distintas de RAG (Retrieval-Augmented Generation)**, Naive RAG, Parent RAG e Rerank RAG

> Todas as abordagens foram usadas para um unico documento PDF, mas variam na estratÃ©gia de recuperaÃ§Ã£o e construÃ§Ã£o da resposta.

---

## ğŸ§  O que Ã© RAG?

RAG combina recuperaÃ§Ã£o de informaÃ§Ãµes (busca semÃ¢ntica com embeddings) com geraÃ§Ã£o de texto (LLM como GPT), para responder perguntas com base em dados externos.

---

## ğŸ”— Tecnologias utilizadas

- ğŸ“˜ [LangChain](https://www.langchain.com/)
- ğŸ§  OpenAI GPT (Chat + Embeddings)
- ğŸ’¾ [ChromaDB](https://www.trychroma.com/) - vector store local
- ğŸ”¤ [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/) - extraÃ§Ã£o do PDF
- ğŸ§± RecursiveCharacterTextSplitter (chunking)

---

## âœ… Estruturas de RAG implementadas

### 1. ğŸ§± Naive RAG (simples)

> Recupera os `top-k` chunks com embeddings e envia diretamente ao LLM.

```mermaid
flowchart TD
    A[Pergunta do UsuÃ¡rio] --> B[Retriever (Top-k Chunks)]
    B --> C[LLM (GPT)]
    C --> D[Resposta Gerada]
```

âœ… **Vantagens**: simples, rÃ¡pido, fÃ¡cil de implementar  
âš ï¸ **LimitaÃ§Ãµes**: chunks isolados podem perder contexto

**Melhorias futuras**:
- Ajuste dinÃ¢mico de `k` com base na complexidade da pergunta
- Filtros por metadado (capÃ­tulo, tema)
- Prompt adaptativo conforme tipo de questÃ£o

---

### 2. ğŸ§© Parent RAG (com contexto estendido)

> Cada chunk pequeno aponta para um "documento pai" maior. ApÃ³s buscar os chunks, recupera os pais para garantir contexto completo.

```mermaid
flowchart TD
    A[Pergunta do UsuÃ¡rio] --> B[Retriever (Chunks Pequenos)]
    B --> C[Mapeamento para Parent ID]
    C --> D[Recupera Texto Completo (Parent)]
    D --> E[LLM (GPT)]
    E --> F[Resposta Gerada]
```

âœ… **Vantagens**: preserva contexto completo de seÃ§Ãµes grandes  
âš ï¸ **LimitaÃ§Ãµes**: pode passar muito conteÃºdo irrelevante ao LLM

**Melhorias futuras**:
- Agrupar parents por similaridade semÃ¢ntica
- Truncamento automÃ¡tico por token mÃ¡ximo
- Cache por parent jÃ¡ usado

---

### 3. ğŸ§  Rerank RAG (relevÃ¢ncia avaliada por LLM)

> Recupera muitos chunks (`top-15~20`), depois o LLM reordena com base em relevÃ¢ncia para a pergunta. Os melhores sÃ£o usados na resposta.

```mermaid
flowchart TD
    A[Pergunta do UsuÃ¡rio] --> B[Retriever (Top-20 Chunks)]
    B --> C[LLM Reranker (Avalia RelevÃ¢ncia)]
    C --> D[Seleciona Top-N Chunks]
    D --> E[LLM (GPT)]
    E --> F[Resposta Gerada]
```

âœ… **Vantagens**: melhora a precisÃ£o ao selecionar sÃ³ os chunks realmente relevantes  
âš ï¸ **LimitaÃ§Ãµes**: mais custo em chamadas ao LLM

**Melhorias futuras**:
- Substituir o reranker por um modelo local (ex: BGE-M3 ou Cohere ReRank)
- OtimizaÃ§Ã£o via batch para scoring paralelo
- Rerank com metadados (capÃ­tulo, personagem, local)

---

## ğŸ“Š Comparativo entre as abordagens

| CritÃ©rio             | Naive RAG | Parent RAG | Rerank RAG |
|----------------------|-----------|------------|------------|
| ImplementaÃ§Ã£o        | âœ… Simples | âš ï¸ Moderada | âš ï¸ Complexa |
| Qualidade de contexto| âŒ Baixa   | âœ… Alta     | âœ… Alta     |
| Custo computacional  | âœ… Baixo   | âš ï¸ MÃ©dio    | âŒ Alto     |
| PrecisÃ£o nas respostas | âš ï¸ VariÃ¡vel | âœ… Boa   | âœ… Ã“tima    |

---

## ğŸš€ PrÃ³ximos passos (gerais)

- Interface Web
- Logging de perguntas e respostas
- AvaliaÃ§Ã£o automÃ¡tica das respostas (score semÃ¢ntico + humano)
- Suporte a perguntas conversacionais (contexto contÃ­nuo)
- Comparador entre as 3 abordagens lado a lado

---

## ğŸ‘¤ Autor

Made with â™¥ by Marcelo Galdino :wave: [Get in touch!](https://www.linkedin.com/in/marcelogaldino/)


## ğŸ“ LicenÃ§a

Este projeto Ã© distribuÃ­do sob a licenÃ§a MIT.